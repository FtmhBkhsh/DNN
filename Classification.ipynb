{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWtrJI8JwUhN"
   },
   "source": [
    "####libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AZNQ05TJwFQK"
   },
   "outputs": [],
   "source": [
    "import torch #import torch library\n",
    "import torch.nn as nn #import torch neural network library\n",
    "import torch.nn.functional as F #import functional neural network module\n",
    "import torch.optim as optim #import optimizer neural network module\n",
    "from torch.autograd import Variable #import variable that connect to automatic differentiation\n",
    "from torchvision import datasets, transforms #import torchvision for datasets and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "c8MhD7I6wYnF"
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DNN, self).__init__() #load super class for training data\n",
    "    self.fc1 = nn.Linear(784, 600)\n",
    "    self.fc2 = nn.Linear(600, 450)\n",
    "    self.fc3 = nn.Linear(450, 200)\n",
    "    self.fc4 = nn.Linear(200, 100)\n",
    "    self.fc5 = nn.Linear(100, 50)\n",
    "    self.fc6 = nn.Linear(50, 20)\n",
    "    self.fc7 = nn.Linear(20, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\t\n",
    "  def forward(self, x): #feed forward\n",
    "    layer1 = x.view(-1, 784) #make it flat from 0 - 320\n",
    "    layer2 = self.relu(self.fc1(layer1)) #layer2 = layer1 -> fc1 -> relu\n",
    "    layer3 = self.relu(self.fc2(layer2)) #layer3 = layer2 -> fc2 -> relu\n",
    "    layer4 = self.relu(self.fc3(layer3)) #layer4 = layer3 -> fc3 -> relu\n",
    "    layer5 = self.relu(self.fc3(layer4)) #layer5 = layer4 -> fc4 -> relu\n",
    "    layer6 = self.relu(self.fc3(layer5)) #layer6 = layer5 -> fc5 -> relu\n",
    "    layer7 = self.relu(self.fc3(layer6)) #layer7 = layer6 -> fc6 -> relu\n",
    "    return F.log_softmax(layer7) #softmax activation to layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "CgVZDZ7twf_1"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\tdef read(self):\n",
    "\t\t#load train and test loader that will be normalized and shuffle\n",
    "\t\ttrain_loader = torch.utils.data.DataLoader( \n",
    "\t\t\tdatasets.MNIST('dataset/',train=True, download=True, \n",
    "\t\t\t\t transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "\t\t\t\t batch_size=1000, shuffle=True)\n",
    "\t\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\t\tdatasets.MNIST('dataset/',train=False, \n",
    "\t\t\t\t transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "\t\t\t\t batch_size=1000, shuffle=True)\n",
    "\t\treturn train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VZ27RhRwdUE",
    "outputId": "364d5284-b968-40f4-933e-382020fa35f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epochs: 0, Loss: 5.322760 \n",
      "Train Epochs: 0, Loss: 4.991116 \n",
      "Train Epochs: 0, Loss: 2.999193 \n",
      "Train Epochs: 0, Loss: 2.260288 \n",
      "Train Epochs: 0, Loss: 1.515417 \n",
      "Train Epochs: 0, Loss: 1.050303 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss: 0.6531, Accuracy: 80\n",
      "Train Epochs: 1, Loss: 0.687634 \n",
      "Train Epochs: 1, Loss: 0.400955 \n",
      "Train Epochs: 1, Loss: 0.433036 \n",
      "Train Epochs: 1, Loss: 0.419620 \n",
      "Train Epochs: 1, Loss: 0.382164 \n",
      "Train Epochs: 1, Loss: 0.312716 \n",
      "\n",
      "Average Loss: 0.3059, Accuracy: 91\n",
      "Train Epochs: 2, Loss: 0.308154 \n",
      "Train Epochs: 2, Loss: 0.308312 \n",
      "Train Epochs: 2, Loss: 0.285749 \n",
      "Train Epochs: 2, Loss: 0.286615 \n",
      "Train Epochs: 2, Loss: 0.296734 \n",
      "Train Epochs: 2, Loss: 0.264316 \n",
      "\n",
      "Average Loss: 0.2610, Accuracy: 93\n",
      "Train Epochs: 3, Loss: 0.270541 \n",
      "Train Epochs: 3, Loss: 0.266482 \n",
      "Train Epochs: 3, Loss: 0.255587 \n",
      "Train Epochs: 3, Loss: 0.276167 \n",
      "Train Epochs: 3, Loss: 0.249886 \n",
      "Train Epochs: 3, Loss: 0.263289 \n",
      "\n",
      "Average Loss: 0.2312, Accuracy: 93\n",
      "Train Epochs: 4, Loss: 0.217255 \n",
      "Train Epochs: 4, Loss: 0.211104 \n",
      "Train Epochs: 4, Loss: 0.189001 \n",
      "Train Epochs: 4, Loss: 0.204439 \n",
      "Train Epochs: 4, Loss: 0.195633 \n",
      "Train Epochs: 4, Loss: 0.201768 \n",
      "\n",
      "Average Loss: 0.2062, Accuracy: 94\n",
      "Train Epochs: 5, Loss: 0.217678 \n",
      "Train Epochs: 5, Loss: 0.204988 \n",
      "Train Epochs: 5, Loss: 0.186490 \n",
      "Train Epochs: 5, Loss: 0.183826 \n",
      "Train Epochs: 5, Loss: 0.218357 \n",
      "Train Epochs: 5, Loss: 0.177209 \n",
      "\n",
      "Average Loss: 0.1846, Accuracy: 95\n",
      "Train Epochs: 6, Loss: 0.176788 \n",
      "Train Epochs: 6, Loss: 0.190192 \n",
      "Train Epochs: 6, Loss: 0.191896 \n",
      "Train Epochs: 6, Loss: 0.200863 \n",
      "Train Epochs: 6, Loss: 0.178991 \n",
      "Train Epochs: 6, Loss: 0.173734 \n",
      "\n",
      "Average Loss: 0.1695, Accuracy: 95\n",
      "Train Epochs: 7, Loss: 0.169780 \n",
      "Train Epochs: 7, Loss: 0.163997 \n",
      "Train Epochs: 7, Loss: 0.166935 \n",
      "Train Epochs: 7, Loss: 0.163115 \n",
      "Train Epochs: 7, Loss: 0.143654 \n",
      "Train Epochs: 7, Loss: 0.164030 \n",
      "\n",
      "Average Loss: 0.1579, Accuracy: 96\n",
      "Train Epochs: 8, Loss: 0.130329 \n",
      "Train Epochs: 8, Loss: 0.166912 \n",
      "Train Epochs: 8, Loss: 0.149349 \n",
      "Train Epochs: 8, Loss: 0.149193 \n",
      "Train Epochs: 8, Loss: 0.149261 \n",
      "Train Epochs: 8, Loss: 0.139388 \n",
      "\n",
      "Average Loss: 0.1455, Accuracy: 96\n",
      "Train Epochs: 9, Loss: 0.115223 \n",
      "Train Epochs: 9, Loss: 0.152232 \n",
      "Train Epochs: 9, Loss: 0.130224 \n",
      "Train Epochs: 9, Loss: 0.128469 \n",
      "Train Epochs: 9, Loss: 0.136847 \n",
      "Train Epochs: 9, Loss: 0.115989 \n",
      "\n",
      "Average Loss: 0.1365, Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "args ={}\n",
    "args['epoch']=10\n",
    "\tmodel = DNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #load optimizer SGD with momentum 0.9 and learning rate 0.01\n",
    "train_loader,test_loader = Dataset().read()\n",
    "for epoch in range(int(args['epoch'])): # train epoch = 10\n",
    "\t\tmodel.train() #training\n",
    "\t\tfor batch_idx, (data, label) in enumerate(train_loader): #enumerate train_loader per batch-> index, (data, label) ex: 0, (img1, 4)... 1, (img2, 2)\n",
    "\t\t\tdata, label = Variable(data), Variable(label) #create torch variable and enter each data and label into it\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput = model(data) #enter data into model, save in output\n",
    "\t\t\ttrain_loss = F.nll_loss(output, label) #nll = negative log likehood loss between output and label. it useful for classification problem with n class\n",
    "\t\t\ttrain_loss.backward() #compute gradient\n",
    "\t\t\toptimizer.step() #update weight\n",
    "\t\t\tif batch_idx % 10 == 0: #display step\n",
    "\t\t\t\tprint('Train Epochs: {}, Loss: {:.6f} '.format(epoch, train_loss.data )) #print\n",
    "\t\tmodel.eval() #evaluation/testing\n",
    "\t\ttest_loss = 0\n",
    "\t\tcorrect = 0\n",
    "\t\tfor data, label in test_loader: #separate data and label\n",
    "\t\t\tdata, label = Variable(data,volatile=True), Variable(label) #create torch variable and enter data and label into it\n",
    "\t\t\toutput = model(data) #enter data into model, save in output\n",
    "\t\t\ttest_loss += F.nll_loss(output, label, size_average=False).data #\n",
    "\t\t\tpred = output.data.max(1, keepdim=True)[1] #prediction result\n",
    "\t\t\tcorrect += pred.eq(label.data.view_as(pred)).cpu().sum() #if label=pred then correct++\n",
    "\t\ttest_loss /= len(test_loader.dataset) #compute test loss\n",
    "\t\tprint('\\nAverage Loss: {:.4f}, Accuracy: {:.0f}'.format(test_loss,  100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
